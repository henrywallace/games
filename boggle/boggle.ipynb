{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Boggle Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the most common words that appear in Boggle boards. There are about $16! \\cdot 6^{16} \\approx 5.9 \\times 10^{25}$ different boards, not taking duplicate letters into account, such that we must recourse to sampling to in order to find word probabilities.\n",
    "\n",
    "Boggle is a game with a square board, with 16 slots for cubes. Each cube has letters on its 6 faces. At the start of each game, the board is shuffled so that each cube is in a random slot, and each cube has a random face showing.\n",
    "\n",
    "The goal of the game is to find as many chains of showing letters as possible on the resultant board. A player's score is based on the number of words that no other player found (larger unique words get higher scores). To be precise, a word chain must be a non-repeating sequence of horizontally, vertically, or diagonally contiguous cubes. Now, the same letter may appear twice in a path, but not the same cube. The words must belong to an agreed upon dictionary, and have some minimum length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Boggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the 16 cubes one will find in a standard Boggle board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cubes = [('m', 'u', 'qu', 'h', 'n', 'i'),\n",
    "         ('n', 'g', 'e', 'e', 'a', 'a'),\n",
    "         ('d', 'r', 'y', 'v', 'l', 'e'),\n",
    "         ('t', 't', 'r', 'e', 'y', 'l'),\n",
    "         ('o', 'a', 'b', 'b', 'o', 'j'),\n",
    "         ('h', 'w', 'v', 'e', 'r', 't'),\n",
    "         ('s', 's', 'o', 'i', 't', 'e'),\n",
    "         ('s', 'h', 'a', 'p', 'c', 'o'),\n",
    "         ('g', 'e', 'w', 'n', 'e', 'h'),\n",
    "         ('u', 'm', 'c', 'o', 't', 'i'),\n",
    "         ('n', 'h', 'n', 'l', 'z', 'r'),\n",
    "         ('t', 'w', 'a', 'o', 't', 'o'),\n",
    "         ('p', 's', 'a', 'f', 'f', 'k'),\n",
    "         ('t', 's', 't', 'i', 'd', 'y'),\n",
    "         ('e', 'r', 'l', 'i', 'x', 'd'),\n",
    "         ('s', 'u', 'e', 'e', 'n', 'i')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the process of shuffling a board, both the slot and face of each cube will be chosen uniformly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import choice, shuffle\n",
    "from itertools import islice\n",
    "\n",
    "def rand_board():\n",
    "    board = []\n",
    "    shuffle(cubes)\n",
    "    cube_order = iter(cubes)\n",
    "    for i in range(4):\n",
    "        row = islice(cube_order, 4)\n",
    "        board.append([choice(cube) for cube in row])\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['d', 's', 'l', 'l'],\n",
       " ['s', 's', 'w', 'n'],\n",
       " ['m', 'j', 'g', 'a'],\n",
       " ['qu', 't', 'v', 'w']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_board()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find viable words on a board, we'll recursively traverse adjoining letters from each of the 16 cube slots. Once a sequence goes out of bounds or repeats a cube, traversal in that direction will stop. \n",
    "\n",
    "Now, at each step in this process we could look up the sequence in a hash table (dictionary). But, it may be the case that the current sequence is no prefix of any dictionary word. So, it will be much more efficient to store the dictionary in a trie. [Here](http://stackoverflow.com/a/11016430) is an inspiring elegant implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_trie(words):\n",
    "    root = {}\n",
    "    for word in words:\n",
    "        node = root\n",
    "        for letter in word:\n",
    "            node = node.setdefault(letter, {})\n",
    "        node[None] = None    # signify that node is a word\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': {'p': {'p': {'l': {'e': {None: None}, 'y': {None: None}}}}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_trie(['apply', 'apple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a recursive function that generates unique words on a given board, of at least some length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def get_words(board, trie, min_len=3):\n",
    "    '''Yield possible words from a given board and trie.\n",
    "    '''\n",
    "    offsets = [(-1, -1), (-1, 0), (-1, 1),\n",
    "               (0,  -1),          (0,  1),\n",
    "               (1,  -1), (1,  0), (1,  1)]\n",
    "    \n",
    "    def recur(i, j, path, seen, node):\n",
    "        if (i, j) in seen:    # path can't cross itself\n",
    "            return\n",
    "        if not (0 <= i < 4) or not (0 <= j < 4):    # path can't wrap around board\n",
    "            return\n",
    "        letter = board[i][j]\n",
    "        path += letter\n",
    "        seen.add((i, j))\n",
    "        if letter in node:    # path is a prefix\n",
    "            if None in node[letter]:    # path is a word\n",
    "                yield path\n",
    "            for i_off, j_off in offsets:    # traverse neighbors even if path is word\n",
    "                yield from recur(i + i_off, j + j_off, path, seen.copy(), node[letter])\n",
    "    \n",
    "    seen = set()\n",
    "    for i, j in product(range(4), range(4)):\n",
    "        for word in recur(i, j, '', set(), trie):\n",
    "            if len(word) >= min_len and word not in seen:\n",
    "                yield word\n",
    "                seen.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding words necessitates a dictionary, and a _Scrabble_-esque dictionary seems fitting. Conveniently, the analagous _Words With Friends_ game's dictionary is [publicly available](http://gaming.stackexchange.com/a/7163): the Enhanced North American Benchmark Lexicon. So I'm going to use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('enable1.txt') as f:\n",
    "    trie = make_trie(word.strip() for word in f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Word Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate word frequencies, we'll gather relative frequencies from random boards. But how many boards to sample from?\n",
    "\n",
    "We can view the appearance of a specific word as a Bernoulli random variable with some unknown probability. We'd like to sample enough boards so that the resultant $k$ most common words is are in fact the most common, and in order.\n",
    "\n",
    "In the land of probability bounds, where Markov and Chebyshev wander wide, one called Hoeffding's inequality is particularly powerful when it comes to Bernoulli variables. It can tell us how tight an $\\epsilon$-sized confidence interval will be with $n$ samples.\n",
    "\n",
    "Specifically, let $X_1, \\ldots, X_n \\sim \\mathrm{Bernoulli}(p)$. Then, for any $\\epsilon > 0$,\n",
    "\n",
    "$$P(\\left| \\overline{X_n} - p \\right| > \\epsilon) \\le 2 e^{-2n\\epsilon^2}$$\n",
    "\n",
    "So, to be $\\alpha$ confident that some variable is within error $\\epsilon$ of it's true value, we'll need\n",
    "\n",
    "$$n \\ge \\frac{1}{2\\epsilon^2}\\log{\\frac{2}{\\alpha}}$$\n",
    "\n",
    "samples.\n",
    "\n",
    "This applies to any Bernoullie variable, and each random Boggle board gives a Bernoulli value to every word (it appears or it doesn't). So, we can just sample a bunch of boards, which will probably produce a tight enough bound.\n",
    "\n",
    "However, we'll have to be very diligent about choosing our confidence $\\alpha$, and tightness $\\epsilon$. We can't guarantee that the top $k$ words are in fact the top $k$, and in order, but it does ossify those top probabilities with a diligently chosen $\\epsilon$.\n",
    "\n",
    "Experimenting with 1000 boards showed that top 10 words' probabilities overlap, suggesting that we need $\\epsilon < 0.001$ to be, say, $\\alpha = 0.1$ confident that the ordering is correct. Unfortunately, this would lead to $n \\ge 1.5 \\times 10^6$ boards, which is too many hours of computation. Instead we'll shoot for just having the same set of top words, rather than correct ranking. The difference in probability between the empirical most common word and the 20th (we choose $k$ arbitrarily) was about 0.01, yielding $n \\ge 14979$ boards, which is more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'tret', 0.12), (2, 'tore', 0.11), (3, 'rete', 0.11), (4, 'tees', 0.1), (5, 'lets', 0.09), (6, 'nets', 0.09), (7, 'rote', 0.09), (8, 'sere', 0.08), (9, 'nose', 0.08), (10, 'site', 0.08), (11, 'tree', 0.08), (12, 'hero', 0.08), (13, 'sone', 0.07), (14, 'ones', 0.07), (15, 'ores', 0.07), (16, 'noes', 0.07), (17, 'seel', 0.07), (18, 'teel', 0.07), (19, 'leet', 0.07), (20, 'lees', 0.07)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# NBOARDS = 14979\n",
    "NBOARDS = 100\n",
    "\n",
    "counts = Counter()\n",
    "for _ in range(NBOARDS):\n",
    "    board = rand_board()\n",
    "    counts += Counter(get_words(board, trie, min_len=4))\n",
    "\n",
    "table = [(i, word, count/NBOARDS) for i, (word, count)\n",
    "         in enumerate(counts.most_common(20), start=1)]\n",
    "\n",
    "print(table)\n",
    "\n",
    "# print(tabulate(table, headers=['Rank', 'Word', 'Frequency'], floatfmt='.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common and Ordinary Word Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common list is interesting, but some of the words are so ordinary that other players will probably know them and also find them. To improve one's chances of winning Boggle, it would be nice to have a ranking that takes into account both a word's probability of occurence, *and ordinariness*.\n",
    "\n",
    "This is reminiscent [tf-idf weighting](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html)! Our current list gives us the *term frequency*, so all that remains is to find each term's *inverse document frequency*. Here, we assume that usage corresponds to how likely it is for a word to be known.\n",
    "\n",
    "We'll use word usage counts from [American National Corpus](http://www.anc.org/data/anc-second-release/frequency-data/). However, some of the words from Boggle might have various endings (e.g. \"apple\", \"apples\"), and those should add in frequency for \"ordinariness.\" So we'll stem the words for lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from collections import defaultdict\n",
    "\n",
    "snow = SnowballStemmer('english')\n",
    "freqs = defaultdict(int)\n",
    "min_freq = 1    # smallest frequency found, for use with Boggle words not in list\n",
    "\n",
    "with open('ANC-token-count.txt', encoding='ISO-8859-1') as f:    # there's weird characters\n",
    "    for word, *_, freq in map(str.split, f):\n",
    "        if word == 'Total':    # there was a total count of words at the end\n",
    "            continue\n",
    "        word, freq = snow.stem(word), float(freq)\n",
    "        freqs[word] += freq\n",
    "        min_freq = freqs[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to calculate the scores of the words output a new sorted list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "tfs, idfs = [], []\n",
    "for word, count in counts.items():\n",
    "    word = snow.stem(word)\n",
    "    tfs.append(1 + log(count))\n",
    "    idfs.append(log(1 + 1/freqs.get(word, min_freq)))\n",
    "\n",
    "scores = sorted([(tf*idf, word) for tf, idf, word in zip(tfs, idfs, counts.keys())], reverse=True)\n",
    "table = [(i, word, score) for i, (score, word) in enumerate(scores[:20], start=1)]\n",
    "print(tabulate(table, headers=['Rank', 'Word', 'Score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Epilogue\n",
    "\n",
    "After completing this task, several sub-topics came to mind. We address some of those here. Unlike in previous sections, we will be lax proving the correct number of boards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Possible Words Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be interesting to see the minimum, maximum, mean, etc. of the number of words found over all boards. For the sake of brevity, rather than fitting the distribution and devising estimators, we'll just plot a simple histogram. And I choose an arbitrarily large number of boards to sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "%matplotlib inline\n",
    "seaborn.set_context('notebook')\n",
    "seaborn.set_style('whitegrid')\n",
    "\n",
    "num_words = []\n",
    "for _ in range(2000):\n",
    "    num_words.append(len(list(get_words(rand_board(), trie))))\n",
    "    \n",
    "plt.hist(num_words, bins=40, normed=True)\n",
    "plt.title('Number of Possible Words Distribution')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Proportion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Proportion of Vowels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A friend gave the comment that the \"best\" Boggle boards are comprised of 40% vowels. Best, here, was intended to mean highest number of word solutions. Now to experimentally prove this claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "NBOARDS = 1000\n",
    "data = defaultdict(int)    # store how many words found per vowel, summed over boards\n",
    "vowels = set('aeiou')    # for simplicity we assume that these are the only vowels\n",
    "\n",
    "for _ in range(NBOARDS):\n",
    "    num_vowels = 0\n",
    "    board = rand_board()\n",
    "    for letter in chain(*board):\n",
    "        if letter in vowels:\n",
    "            num_vowels += 1\n",
    "    num_words = len(list(get_words(board, trie, min_len=4)))\n",
    "    data[num_vowels] += num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to plot the mean number of found words per number of vowels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = zip(*data.items())\n",
    "\n",
    "plt.plot(x, [t/NBOARDS for t in y], 'bo:')\n",
    "plt.title(r'Number of Words in Board per Vowel')\n",
    "plt.xlabel('Number of vowels')\n",
    "plt.ylabel('Total number of found words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "6/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would you look at that, 40% was pretty spot on for an off the cuff comment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
